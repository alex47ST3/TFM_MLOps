{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer\n",
    "import classes.utils as utils\n",
    "from classes.splitter import Splitter\n",
    "\n",
    "\n",
    "utils.set_parent_directory_as_working_directory()\n",
    "\n",
    "# TODO: Move this to a config file\n",
    "# Importing\n",
    "DATA_FOLDER = \"./data\"\n",
    "\n",
    "\n",
    "FE_DATA_PATH = DATA_FOLDER +'/fe_data.csv'\n",
    "DATES_DATA_PATH = DATA_FOLDER +'/dates_data.csv'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Introduction\n",
    "In this notebook we will develop our first model. We are going to assume that we have 1 year of loans that have finished (finished_d = issued date + total length of loan), which basically place ourselves on 2011-05-01.\n",
    "\n",
    "\n",
    "# 1 Splitting data\n",
    "Before we get hands on with the modelling, we need to split the data into train and test sets. As we mentioned in the preprocessing notebook we will be using the create variable  'finished_d' to \n",
    "\n",
    "We will use the train set to train the model and the test set to evaluate the model. We will use the train_test_split function from sklearn to split the data. We will use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from ./data/fe_data.csv\n",
      "Dates data loaded from ./data/dates_data.csv\n",
      "Date column finished_d added to the data\n",
      "Data filtered by 12 months\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\TFM\\TFM_MLOps\\mlflow_project\\src\\classes\\splitter.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.drop(columns=self.column_to_split_by, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test and train attributes defined 0.3.\n",
      "        Test size: 597\n",
      "        Train size: 1393\n"
     ]
    }
   ],
   "source": [
    "splitter_name = \"splitter\"\n",
    "\n",
    "splitter = Splitter(\n",
    "    name = splitter_name\n",
    "    , data_path = FE_DATA_PATH\n",
    "    , date_cols = []\n",
    "    , target_variable = 'loan_status'\n",
    "    , destination_directory = DATA_FOLDER\n",
    "    , dates_data_path = DATES_DATA_PATH\n",
    "    , column_to_split_by = 'finished_d'\n",
    "    , test_size = 0.3\n",
    "    , random_state = 47\n",
    ")\n",
    "\n",
    "splitter.execute()\n",
    "splitter.split_data_filtered(number_of_months=12)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object contains x_train, x_test, y_train and y_test, for the first 12 months, later we can try with more months bu just changing \"number_of_months\".\n",
    "\n",
    "# 2 Modelling first year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_grid_search(model, param_distributions, X_train, y_train,  random_state):\n",
    "    \"\"\"\n",
    "    Perform a random grid search using scikit-learn's RandomizedSearchCV class.\n",
    "\n",
    "    Parameters:\n",
    "    - model: a scikit-learn model object\n",
    "    - param_distributions: a dictionary containing hyperparameter names and distributions\n",
    "    - X_train: training input data\n",
    "    - y_train: training output data\n",
    "    - n_iter: number of parameter settings that are sampled\n",
    "    - cv: number of cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    - best_estimator: the best estimator found during the search\n",
    "    - best_params: the hyperparameters of the best estimator\n",
    "    \"\"\"\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state= random_state)\n",
    "    # Create a RandomizedSearchCV object\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        model, param_distributions, cv=cv, scoring='recall', n_iter=10)\n",
    "\n",
    "    # Fit the RandomizedSearchCV object to the data\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and score\n",
    "    print(\"Best parameters: {}\".format(search.best_params_))\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(search.best_score_))\n",
    "\n",
    "    # Get the best estimator and its parameters\n",
    "    best_estimator = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    return best_estimator, best_params\n",
    "\n",
    "def cross_val_train_predict(model, X_train, y_train, X_test, scoring_metric, seed=SEED):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "    scores = cross_val_score(model, X_train, y_train,\n",
    "                             scoring=scoring_metric, cv=cv, n_jobs=-1)\n",
    "    print('Mean Recall in Train: %.3f (%.3f)' %\n",
    "          (np.mean(scores), np.std(scores)))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/30 11:56:55 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def set_experiment(experiment_name):\n",
    "    # Set the experiment name and tracking URI\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    mlflow.set_tracking_uri('http://localhost:5000')\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    run = client.create_run(experiment.experiment_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(experiment_name\n",
    "                   , model_class\n",
    "                   , params\n",
    "                   , splitter\n",
    "                   , log_model=False\n",
    "                   , params_distributions=None\n",
    "                   , random_satate=47):\n",
    "    \n",
    "    set_experiment(experiment_name)\n",
    "    mlflow.sklearn.autolog(log_models=log_model)\n",
    "    \n",
    "    # Log the evaluation metrics and model parameters in MLflow\n",
    "    with mlflow.start_run():\n",
    "        model = model_class(**params)  # create a new instance of the model class\n",
    "        model.fit(splitter.X_train, splitter.y_train)\n",
    "        y_pred = model.predict(splitter.X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_experiment('First year'\n",
    "               , DecisionTreeClassifier\n",
    "               , {'max_depth': 5\n",
    "                  , 'random_state' : 47}\n",
    "               , splitter\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RestException",
     "evalue": "RESOURCE_DOES_NOT_EXIST: No Experiment with id=1 exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     experiment \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mget_experiment_by_name(experiment_name)\n\u001b[0;32m     11\u001b[0m     client\u001b[39m.\u001b[39mdelete_experiment(experiment\u001b[39m.\u001b[39mexperiment_id)\n\u001b[1;32m---> 13\u001b[0m delete_experiment(\u001b[39m'\u001b[39;49m\u001b[39mbase model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mdelete_experiment\u001b[1;34m(experiment_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     10\u001b[0m experiment \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mget_experiment_by_name(experiment_name)\n\u001b[1;32m---> 11\u001b[0m client\u001b[39m.\u001b[39;49mdelete_experiment(experiment\u001b[39m.\u001b[39;49mexperiment_id)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\.conda\\envs\\tfm\\lib\\site-packages\\mlflow\\tracking\\client.py:540\u001b[0m, in \u001b[0;36mMlflowClient.delete_experiment\u001b[1;34m(self, experiment_id)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdelete_experiment\u001b[39m(\u001b[39mself\u001b[39m, experiment_id: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[39m    Delete an experiment from the backend store.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39m    This deletion is a soft-delete, not a permanent deletion.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[39m        Lifecycle_stage: deleted\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mdelete_experiment(experiment_id)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\.conda\\envs\\tfm\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:246\u001b[0m, in \u001b[0;36mTrackingServiceClient.delete_experiment\u001b[1;34m(self, experiment_id)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdelete_experiment\u001b[39m(\u001b[39mself\u001b[39m, experiment_id):\n\u001b[0;32m    241\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[39m    Delete an experiment from the backend store.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \n\u001b[0;32m    244\u001b[0m \u001b[39m    :param experiment_id: The experiment ID returned from ``create_experiment``.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mdelete_experiment(experiment_id)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\.conda\\envs\\tfm\\lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:113\u001b[0m, in \u001b[0;36mRestStore.delete_experiment\u001b[1;34m(self, experiment_id)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdelete_experiment\u001b[39m(\u001b[39mself\u001b[39m, experiment_id):\n\u001b[0;32m    112\u001b[0m     req_body \u001b[39m=\u001b[39m message_to_json(DeleteExperiment(experiment_id\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(experiment_id)))\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(DeleteExperiment, req_body)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\.conda\\envs\\tfm\\lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:56\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[1;34m(self, api, json_body)\u001b[0m\n\u001b[0;32m     54\u001b[0m endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[0;32m     55\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\.conda\\envs\\tfm\\lib\\site-packages\\mlflow\\utils\\rest_utils.py:281\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[1;34m(host_creds, endpoint, method, json_body, response_proto)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     response \u001b[39m=\u001b[39m http_request(\n\u001b[0;32m    279\u001b[0m         host_creds\u001b[39m=\u001b[39mhost_creds, endpoint\u001b[39m=\u001b[39mendpoint, method\u001b[39m=\u001b[39mmethod, json\u001b[39m=\u001b[39mjson_body\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 281\u001b[0m response \u001b[39m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[0;32m    282\u001b[0m js_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n\u001b[0;32m    283\u001b[0m parse_dict(js_dict\u001b[39m=\u001b[39mjs_dict, message\u001b[39m=\u001b[39mresponse_proto)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\.conda\\envs\\tfm\\lib\\site-packages\\mlflow\\utils\\rest_utils.py:207\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[1;34m(response, endpoint)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m _can_parse_as_json_object(response\u001b[39m.\u001b[39mtext):\n\u001b[1;32m--> 207\u001b[0m         \u001b[39mraise\u001b[39;00m RestException(json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext))\n\u001b[0;32m    208\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m         base_msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAPI request to endpoint \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m failed with error code \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m != 200\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    210\u001b[0m             endpoint,\n\u001b[0;32m    211\u001b[0m             response\u001b[39m.\u001b[39mstatus_code,\n\u001b[0;32m    212\u001b[0m         )\n",
      "\u001b[1;31mRestException\u001b[0m: RESOURCE_DOES_NOT_EXIST: No Experiment with id=1 exists"
     ]
    }
   ],
   "source": [
    "def delete_experiment(experiment_name):\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    # Checks if the experiment exists\n",
    "    if experiment is None:\n",
    "        print(f\"Experiment '{experiment_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    client.delete_experiment(experiment.experiment_id)\n",
    "\n",
    "delete_experiment('base model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
